\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{multicol}
\usepackage{upquote}
\usepackage{url}
\newcommand{\class}{miniHive}
\newcommand{\term}{}
\newcommand{\examnum}{Milestone 3}
\usepackage{paralist}
\usepackage{fancyvrb}


\pagestyle{head}
\firstpageheader{}{}{}
\runningheader{\class}{\examnum\ - Page \thepage\ of \numpages}{}
\runningheadrule



\begin{document}

\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{6pt}} l}
\textbf{\class} & \\
\textbf{\term} &&\\
\textbf{\examnum} &&\\
\end{tabular*}\\
\rule[2ex]{\textwidth}{2pt}

\noprintanswers

\noindent


\section*{Compiling Relational Algebra to MapReduce Jobs}

In the third milestone, we compile relational algebra queries into a physical query plan of MapReduce jobs. The MapReduce jobs can then be executed directly on Hadoop.



\begin{questions}

\question
Read the chapter on ``Workflow Systems" for MapReduce engines in chapter~2.4.1
of the book ``Mining Massive Datasets''. The Python module \verb!luigi! is such a workflow engine that can execute MapReduce jobs (among many other things). {\bf \verb!luigi! is already installed in the course VM}.

A good starting point are the \verb!luigi! examples on GitHub: \url{https://github.com/spotify/luigi/tree/master/examples}, e.g.\ the \verb!hello_world.py! file.

If you are interested in details, our setup is inspired by \verb!luigi/examples/wordcount_hadoop.py!. However, {\bf you are not expected to dig deep into \verb!luigi!}. Understanding (and appreciating) {\em what}\/ it does for you should be enough.

If you want to code outside of the VM, on our personal computing device, make sure you have \verb!luigi! installed. 


\question 
In the VM, you need a file \verb!luigi.cfg! in the folder where you are running your luigi tasks, with these contents:  
\begin{Verbatim}[frame=single,fontsize=\small]
[hadoop]
streaming-jar=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar
python-executable=/usr/local/bin/python3.6
\end{Verbatim}


\question Add code to the provided Python module \verb!ra2mr! which compiles a relational algebra query into a MapReduce workflow. We make the following simplifying assumptions:

\begin{itemize}
    \item 

We assume that the queries are the output of milestone~2 and therefore use the operators $\sigma$, $\pi$, $\rho$, and $\bowtie_p$ (Equi-join) only. (We do not implement the cross product as a MapReduce job, it doesn't make much sense for {\em big}\/ data.)

\item
We assume that our input consists of ``flat'' JSON documents (no arrays or  nested objects). Here is the data for  relation \verb!Person! from the {\em pizza}\/ example. Each line is a key-value pair, separated by a tab. The key is the relation name, the value is a flat JSON document.

\begin{Verbatim}[frame=single,fontsize=\scriptsize]
Person	{"Person.name": "Amy", "Person.age": 16, "Person.gender": "female"}
Person	{"Person.name": "Ben", "Person.age": 21, "Person.gender": "male"}
...
\end{Verbatim}


\end{itemize}


In Moodle, you find the skeleton code for \verb!ra2mr.py!. You only need to flesh out the code for Mapper and Reducer functions. The boilerplate code for building workflows with \verb!luigi! is already provided!

A task parameter of type \verb!ra2mr.ExecEnv! controls the execution environment:

\begin{compactitem}

    \item Set the task parameter \verb!exec_environment! to \verb!HDFS! to run tasks on Hadoop (the VM). This assumes that all input files reside in HDFS.
    
    This is our {\em production mode}\/. Unless you are willing to endure long waits, this mode is unsuitable for development. Use \verb!LOCAL! for development, as described next.
    
    \item Set the task parameter \verb!exec_environment! to \verb!LOCAL! to run tasks without HDFS or Hadoop involved (or even installed)
    The pizza files are provided in Moodle.
    
    This is intended as the {\em development mode}: You will have quick turnarounds and can easily inspect any intermediate data written to temporary files.
    
    \item The unit tests set the task parameter \verb!exec_environment! to \verb!MOCK!. All files are then kept in main memory only.
    This is intended for {\em unit testing}.
    
\end{compactitem}




This is how you would interact with \verb!ra2mr! from within Python code:

\begin{Verbatim}[frame=single, fontsize=\small]
import luigi
import radb
import ra2mr

# Take a relational algebra query...
raquery = radb.parse.one_statement_from_string("\project_{name} Person;")

# ... translate it into a luigi task encoding a MapReduce workflow...
task = ra2mr.task_factory(raquery, env=ra2mr.ExecEnv.HDFS)

# ... and run the task on Hadoop, using HDFS for input and output:
# (for now, we are happy working with luigi's local scheduler).
luigi.build([task], local_scheduler=True)

\end{Verbatim}


You can also execute \verb!luigi! tasks from the command-line, as also described here~\url{https://luigi.readthedocs.io/en/stable/running_luigi.html}.
This is great for development and manual testing.

For instance, to evaluate a selection query locally on the VM, you can write

\begin{Verbatim}[frame=single, fontsize=\small]
python3.6 ra2mr.py SelectTask \
--querystring "\select_{gender='female'} Person;" \
--exec-environment LOCAL --local-scheduler
\end{Verbatim}

or alternatively

\begin{Verbatim}[frame=single, fontsize=\small]
PYTHONPATH=. luigi --module ra2mr SelectTask \ 
--querystring "\select_{gender='female'} Person;" \ 
--exec-environment LOCAL --local-scheduler
\end{Verbatim}

Inspect the \verb!*.tmp!-files for intermediate results and the final output.
Remember to clear any output files before starting the next task,
since \verb!luigi! will not recompute them.

Similarly, to evaluate a projection query locally on the VM,
you can write

\begin{Verbatim}[frame=single, fontsize=\small]
python3.6 ra2mr.py ProjectTask \ 
--querystring "\project_{name} Person;" \ 
--exec-environment LOCAL --local-scheduler
\end{Verbatim}

To run the queries on Hadoop, simply switch to the \verb!HDFS! environment. Make sure that all required input files have been loaded into HDFS, and that any previous output has been cleared. (If one way of calling the tasks on Hadoop doesn't work, try the other one. Currently, I cannot explain why there are sometimes issues.)

\question
Combine your code of all three milestones, to execute SQL queries in {\em miniHive}\/. The unit tests in \verb!test_e2e.py! check this for you.

\end{questions}


\bigskip
We will use special unit test modules called \verb!pytest! and \verb!pytest-repeat!. This allows us to repeat unit tests, to check for ``flaky'' tests. Tests may fail sporadically, often due to changes in the execution order, most likely when you compute the join. To check for flakiness, the unit tests in \verb!test_ra2mr.py! will be run several times. (This is no guarantee, of course, for the absence of flakiness.)
        If you want to check this yourself, install \verb!pytest! and \verb!pytest-repeat! on your VM using \verb!pip!.
        
\section{Material in Moodle}
\begin{compactitem}
\item
The skeleton code for \verb!ra2mr.py!.

\item
The file \verb!test_e2e.py! contains unit tests. Make sure your implementation passes these tests.

\item
The file \verb!test_ra2mr.py! contains unit tests. Make sure your implementation passes these tests.

\end{compactitem}

\section{What to Submit}
\begin{compactitem}
\item
Submit \verb!ra2mr.py! with your extensions/implementation,

\item
and your implementation of the previous milestones. 
\end{compactitem}

\bigskip
\noindent
\rule{\linewidth}{0.5mm}

{\bf Remarks:}
For Milestone 3, you are not asked to find any particular optimizations to make your implementation more efficient. All you are required to provide is a correct and clean implementation.

\noindent
\rule{\linewidth}{0.5mm}

\end{document}
